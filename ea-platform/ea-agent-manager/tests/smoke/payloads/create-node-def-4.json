{
    "type": "worker.inference.llm.openai",
    "name": "OpenAI LLM Inference",
    "creator": "<UUID OF CREATOR USER>",
    "api": {
      "base_url": "https://api.openai.com",
      "endpoint": "/v1/chat/completions",
      "method": "POST",
      "headers": {
        "Content-Type": "application/json",
        "Authorization": "Bearer ((openai_api_key))"
      }
    },
    "parameters": [
      {
        "key": "model",
        "type": "string",
        "description": "Name of the model to use.",
        "enum": ["gpt-4o-mini", "gpt-4o", "o1", "o1-mini", "o3-mini"],
        "default": "gpt-4o-mini"
      },
      {
        "key": "messages",
        "type": "array",
        "description": "Prompts (user, developer, system) to be sent to the model.",
        "default": [
            {
                "role": "user",
                "content": "Hello!"
            }
        ] 
      },
      {
        "key": "temperature",
        "type": "number",
        "description": "Controls randomness in generation (0.0 - 1.0).",
        "default": 0.7
      }
    ],
    "outputs": [
      {
        "key": "textoutput",
        "type": "string",
        "description": "the result of the prompt",
        "default": "someoutput"
      }
    ],
    "metadata": {
      "description": "Makes an inference call to OpenAI for text generation.",
      "tags": ["worker", "llm", "openai", "inference"],
      "additional": {
        "documentation_url": "https://openai.com/api/"
      }
    }
  }
  