---
ollama:
  models:
    pull:
      - llama3.2
      # - deepseek-r1:8b
    run:
      - llama3.2
      # - deepseek-r1:8b

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

ingress:
  enabled: true
  className: "nginx"
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - host: ollama.ea.erulabs.local
      paths:
        - path: /
          pathType: Prefix
