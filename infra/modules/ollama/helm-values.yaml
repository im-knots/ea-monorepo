---
ollama:
  models:
    pull:
      - llama3.2
      # - llama3.3
      # - deepseek-r1:1.5b
      # - deepseek-r1:8b
    run:
      # - llama3.2
      # - llama3.3
      # - deepseek-r1:1.5b
      # - deepseek-r1:8b

    create:
     - name: llama3.2-ctx
       template: |
         FROM llama3.2
         PARAMETER num_ctx 32768
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1
    nvidiaResource: "nvidia.com/gpu"



autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

ingress:
  enabled: true
  className: "nginx"
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - host: ollama.ea.erulabs.local
      paths:
        - path: /
          pathType: Prefix
